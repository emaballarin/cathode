{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kcZNmfJVvLxB"
   },
   "source": [
    "# Using [*Augmented*](https://arxiv.org/abs/1904.01681) [*Neural ODEs*](https://arxiv.org/abs/1806.07366) for ***real-world*** [irregularly-sampled time-series](https://arxiv.org/abs/1907.03907) prediction and simulation.\n",
    "\n",
    "\n",
    "A joint effort of\n",
    "\n",
    "* [Arianna Tasciotti](https://github.com/ariannatasciotti)  \n",
    "* [Milton Nicol√°s Plasencia Palacios](https://github.com/nickplas)  \n",
    "* [Emanuele Ballarin](https://ballarin.cc/)\n",
    "\n",
    "and probably part of [this GitHub repository](https://github.com/emaballarin/cathode).  \n",
    "\n",
    "An upfront *thank you* to [Davide Scassola](https://github.com/DavideScassola), for having made this task a task worth trying to attack. *Per adversa, ultra adversa.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nu0mzA6WvRNA"
   },
   "source": [
    "**NOTICE:**  \n",
    "This Colab/Jupyter notebook just contains the *neural network part* of the analysis. The starting point for this notebook is an `HDF5` file containing the preprocessed dataframe, ready to be used. There should be a notebook ready just for that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKK44fu0vqb9"
   },
   "source": [
    "## Package installation\n",
    "Work-around *Colab* incompatibilities.  \n",
    "*(and remember to restart the runtime at the end!)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_oBjgHp0vpqM"
   },
   "source": [
    "## Reproducibility & compatibility information\n",
    "\n",
    "A little help to foster code reproducibility, courtesy of [Sebastian Raschka](http://sebastianraschka.com/)'s [`Watermark`](https://github.com/rasbt/watermark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6T29iqz51lZd"
   },
   "outputs": [],
   "source": [
    "# Be nice, be reproducible!\n",
    "%reload_ext watermark\n",
    "%watermark -a 'A. Tasciotti, M.N. Plasencia Palacios, and E. Ballarin' -v -w -p numpy,vaex,torch,torchdiffeq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Qg6DwzVvvS2"
   },
   "source": [
    "## Modules\n",
    "\n",
    "Keeping all imported modules in one single place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DOzt_-DivwL0"
   },
   "outputs": [],
   "source": [
    "## IMPORTS ##\n",
    "\n",
    "# Numpy first! (required to force an Intel OpenMP threading layer)\n",
    "import numpy as np\n",
    "\n",
    "# Basics\n",
    "import os\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# Data handling\n",
    "import vaex as vx\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Neural Networks / Neural ODEs\n",
    "import torch\n",
    "import torchdiffeq as thdeq\n",
    "import torch.nn as nn\n",
    "from torchdiffeq import odeint\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "import torch.utils.data.dataloader as DataLoader\n",
    "from torch import optim\n",
    "import torch.cuda.amp as amp\n",
    "from torch._six import inf\n",
    "from torchsummary import summary\n",
    "\n",
    "# Self-rolled utility functions\n",
    "from src.util.datamanip import data_by_tick, data_by_tick_col\n",
    "from src.util.plotting import data_timeplot\n",
    "from src.util.datasets import StockDataset\n",
    "from pyromaniac.optim.torch.adamwcd import AdamWCD as AdamWCD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e3pzoYCHOQJC"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wpr2AtiKv0LN"
   },
   "source": [
    "## Data import\n",
    "\n",
    "Starting from where we left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7DQHr5--HUzN"
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "batch_size = 64\n",
    "input_datasize = 1\n",
    "hidden_size_gruode = 16*(input_datasize+1)\n",
    "hidden_size_ffw_f = 180\n",
    "hidden_size_ffw_g = 30\n",
    "hidden_size_ffw_onn = 30\n",
    "output_datasize = input_datasize\n",
    "ttsr = 0.9\n",
    "window_size = 1000\n",
    "max_train_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOB_tK0n1tfC"
   },
   "outputs": [],
   "source": [
    "class f(torch.nn.Module):\n",
    "  def __init__(self, size, hidden_size):\n",
    "    super(f, self).__init__()\n",
    "    self.fc1 = nn.Linear(size+1, hidden_size)\n",
    "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.fc3 = nn.Linear(hidden_size, size)\n",
    "\n",
    "  def forward(self, t, data):\n",
    "    x = torch.cat((torch.tensor([t],requires_grad=True).to(device),data.to(device)), dim=0).to(device)\n",
    "    x.retain_grad()\n",
    "    x = self.fc1(x)\n",
    "    x = torch.tanh(x)\n",
    "    x = self.fc2(x)\n",
    "    x = torch.tanh(x)\n",
    "    x = self.fc3(x)\n",
    "    x = torch.tanh(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mkN8Atxol7MC"
   },
   "outputs": [],
   "source": [
    "class CustomOdeint(torch.nn.Module):\n",
    "  def __init__(self, size, hidden_size):\n",
    "    super(CustomOdeint, self).__init__()\n",
    "    self.size = size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.f_forward = f(self.size, self.hidden_size).to(device)\n",
    "\n",
    "  def forward(self, y0, t, rtol=1e-7, atol=1e-9, method=None, options=None):\n",
    "    b_size = len(y0)\n",
    "    assert b_size == len(t)\n",
    "    output = torch.Tensor(b_size,t.size()[1],self.size).to(device)\n",
    "    for i in range(0,b_size):\n",
    "      output[i] = odeint(self.f_forward, y0[i], t[i], rtol, atol, method, options)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u8W4pFQxEVnW"
   },
   "outputs": [],
   "source": [
    "customodeint = CustomOdeint(hidden_size_gruode,hidden_size_ffw_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KUvwLbz-mBZk"
   },
   "outputs": [],
   "source": [
    "class GRUODE(torch.nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, bias=True):  # hidden_size == output_size of Sampler class\n",
    "    super(GRUODE, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.bias = bias\n",
    "    self.GRU = torch.nn.GRUCell(self.input_size, self.hidden_size, self.bias).to(device)\n",
    "    self.LN = torch.nn.LayerNorm(hidden_size)\n",
    "\n",
    "\n",
    "  def forward(self, x, hidden, times):\n",
    "    # GRU training algorithm\n",
    "    h_n = customodeint(hidden.to(device), times.to(device), rtol = 1e-3, atol = 1e-4, method='dopri5')[:,1] # check\n",
    "    hy = self.LN(h_n)\n",
    "    hy = self.GRU(x, h_n)\n",
    "    return hy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6ZAR0S9275J"
   },
   "outputs": [],
   "source": [
    "class g(torch.nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size=2):  # input_size == hidden_size of GRUODE\n",
    "    super(g, self).__init__()\n",
    "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "    self.BN =  nn.BatchNorm1d(hidden_size)\n",
    "    self.output_size = output_size\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.fc1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.fc2(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.BN(x)\n",
    "    x = self.fc3(x)\n",
    "    #if x[1] <= 0:\n",
    "      #x = torch.tensor([x[0],-x[1]], requires_grad=True).to(device)\n",
    "    x = torch.cat((x[:,0], torch.abs(x[:,1])), dim=0).to(device).view(-1, self.output_size)\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDf7RVZwmHBf"
   },
   "outputs": [],
   "source": [
    "class Sampler(torch.nn.Module):\n",
    "  def __init__(self, dist, nr_parameters, output_size):             # dim nr_parameters != dim output_size\n",
    "    super(Sampler, self).__init__()\n",
    "    self.dist = dist\n",
    "    self.nr_parameters = nr_parameters\n",
    "    self.output_size = output_size\n",
    "\n",
    "  def forward(self, parameters):\n",
    "    parameters = parameters.t()\n",
    "    parlist = parameters.chunk(self.nr_parameters)\n",
    "    distribution = self.dist(*parlist)                                # unpacking elements of parlist\n",
    "    out =  distribution.rsample(torch.tensor([self.output_size]))[:,0]     # non vuole requires_grad()\n",
    "    out = out.t()                                                     \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjAFdGGEmLJV"
   },
   "outputs": [],
   "source": [
    "class OutputNN(torch.nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size): # dim input_size == hidden_size of GRUODE, dim output_size == dim predictions\n",
    "    super(OutputNN, self).__init__()\n",
    "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.fc1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.fc2(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.fc3(x)\n",
    "    return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zc1bClLcmSAU"
   },
   "outputs": [],
   "source": [
    "class RNNVAEODE(torch.nn.Module):\n",
    "  def __init__(self, input_size, hidden_size_gruode, hidden_size_ffw_g, hidden_size_ffw_onn, prior, h0): \n",
    "    super(RNNVAEODE, self).__init__()\n",
    "\n",
    "    self.h0 = h0\n",
    "    self.h0_buffer = h0\n",
    "    self.GRUODE = GRUODE(input_size, hidden_size_gruode, bias=True)\n",
    "    self.g = g(hidden_size_gruode, hidden_size_ffw_g ,output_size=2) \n",
    "    self.Sampler = Sampler(prior, 2, hidden_size_gruode)\n",
    "    self.OutputNN = OutputNN(hidden_size_gruode, hidden_size_ffw_onn, input_size)\n",
    " \n",
    "  def set_h0(self, new_h0):\n",
    "    self.h0 = new_h0.detach().clone()\n",
    "    self.h0_buffer = new_h0.detach().clone()\n",
    "\n",
    "  def forward(self, past, t_future):\n",
    "\n",
    "    self.h0 = self.h0_buffer.detach().clone()\n",
    "\n",
    "    if past is not None:  #training\n",
    "      past_time = past[:,:,:1]\n",
    "      past_data = past[:,:,1:]\n",
    "      \n",
    "\n",
    "      h_new = self.GRUODE(past_data[:,0],self.h0, torch.cat((past_time[:,0]-torch.ones(past_time[:,0].size()).to(device), past_time[:,0]), dim=1).to(device))\n",
    "      h_prev = h_new\n",
    "\n",
    "      for i in range(1,len(past)):\n",
    "        h_new = self.GRUODE(past_data[:,i], h_prev, torch.cat((past_time[:,i-1], past_time[:,i]), dim=1).to(device))\n",
    "        h_prev = h_new\n",
    "\n",
    "      self.h0_buffer = h_prev.detach().clone()\n",
    "\n",
    "    else:\n",
    "      h_prev = self.h0                        # not rolled\n",
    "\n",
    "    if t_future is None:                      \n",
    "      return None                             # not rolled\n",
    "\n",
    "\n",
    "    param = self.g(h_prev)\n",
    "    z0 = self.Sampler(param)\n",
    "    out = customodeint(z0, t_future, rtol = 1e-3, atol = 1e-4, method='dopri5')[:,:]\n",
    "    output = self.OutputNN(out)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyiP0JTNHzl-"
   },
   "outputs": [],
   "source": [
    "# INITIAL HIDDEN STATE\n",
    "h0 = torch.zeros(batch_size, hidden_size_gruode)\n",
    "h0.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6spucwxast1M"
   },
   "outputs": [],
   "source": [
    "h0_test = torch.zeros(1, hidden_size_gruode)\n",
    "h0_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "91OxhwtGmiv9"
   },
   "outputs": [],
   "source": [
    "mydataset = StockDataset(\"./data/WIKI_PRICES_QUANDL.hdf5\", 'AAPL', 'close', ttsr, window_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ITSRh9CCObZ_"
   },
   "outputs": [],
   "source": [
    "mydataset_test = StockDataset(\"./data/WIKI_PRICES_QUANDL.hdf5\", 'AAPL', 'close', ttsr, window_size, train=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NaSsc8vbO6sj"
   },
   "outputs": [],
   "source": [
    "model = RNNVAEODE(input_datasize, hidden_size_gruode, hidden_size_ffw_g, hidden_size_ffw_onn, distributions.Normal, h0)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5LNs1MVGO-Bj"
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(mydataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUGHgoZ0OnjL"
   },
   "outputs": [],
   "source": [
    "dataloader_test = torch.utils.data.DataLoader(mydataset_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6R-gJMc-rFmA"
   },
   "outputs": [],
   "source": [
    " def param_gn(parameters, norm_type=2): \n",
    "  if isinstance(parameters, torch.Tensor):\n",
    "      parameters = [parameters]\n",
    "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "  norm_type = float(norm_type)\n",
    "  if len(parameters) == 0:\n",
    "      return torch.tensor(0.)\n",
    "  device = parameters[0].grad.device\n",
    "  if norm_type == inf:\n",
    "      total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "  else:\n",
    "      total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "  return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C3UH_e9wUR5M"
   },
   "outputs": [],
   "source": [
    "def get_params_num(net):\n",
    "    return sum(map(torch.numel, net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLi_vK2yPHIf"
   },
   "outputs": [],
   "source": [
    "print(get_params_num(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iu2ANDgWPWcF"
   },
   "outputs": [],
   "source": [
    "# TRAINING LOOP \n",
    "lr = 0.00205\n",
    "#momentum = 0.9\n",
    "eps = 1e-8\n",
    "lrd = 1.0\n",
    "cn = 63*batch_size\n",
    "out_cn = 3*cn\n",
    "wd = 0.005\n",
    "epochs = 20\n",
    "\n",
    "n_batches = len(dataloader)\n",
    "\n",
    "criterion = nn.SmoothL1Loss(reduction = 'sum')\n",
    "#criterion = nn.MSELoss()\n",
    "#optimizer = AdamWCD(model.parameters(), lr=lr, eps=eps, clip_norm=cn, lrd=lrd)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, eps=eps, weight_decay=wd)\n",
    "\n",
    "model.train() \n",
    "for e in range(epochs):\n",
    "  for i,dictionary in enumerate(dataloader):\n",
    "    clip_iter = 3\n",
    "    optimizer.zero_grad()\n",
    "    data_in = dictionary[\"past\"].float().to(device)\n",
    "    data_out_time = dictionary[\"future\"][:,:,:1].float().to(device)\n",
    "    data_out_data = dictionary[\"future\"][:,:,1:].float().to(device)\n",
    "    # tutto quello che viene valutato nel forward viene castato a float.32, \n",
    "    # mentre tutte le cose che vengono calcolate solo una volta (gradienti...) vengono castati a float.16\n",
    "    #with torch.cuda.amp.autocast(enabled=True):                   \n",
    "    outputs = model(data_in,data_out_time)\n",
    "    loss = criterion(outputs, data_out_data)\n",
    "    #optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    paramgn = param_gn(model.parameters())\n",
    "    print(\"gradient_norm: \", paramgn.item())\n",
    "    if (paramgn > out_cn):\n",
    "      clip_iter = 2 \n",
    "    if (i % clip_iter == 0):\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), cn)\n",
    "    optimizer.step()\n",
    "    if i % 1 == 0:\n",
    "        print(\"[EPOCH]: {}, [BATCH]: {}/{}, [LOSS]: {}\".format(e, i, n_batches, loss.item()))\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"RNNVAEODE_bkp.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sMbsy8xeEuct"
   },
   "outputs": [],
   "source": [
    "model_test = model\n",
    "model_test.eval() \n",
    "model_test.set_h0(h0_test)\n",
    "criterion = nn.MSELoss()\n",
    "for i,dictionary in enumerate(dataloader_test):\n",
    "  if dictionary[\"future\"] == []:\n",
    "    data_in = dictionary[\"past\"].float().to(device)\n",
    "    data_out_time = None\n",
    "    outputs = model_test(data_in,data_out_time)\n",
    "  else:\n",
    "    print(dictionary[\"future\"][:,:,:1])\n",
    "    data_out_time = dictionary[\"future\"][:,:,:1].float().to(device)\n",
    "    data_out_data = dictionary[\"future\"][:,:,1:].float().to(device) \n",
    "    outputs = model_test(data_in,data_out_time)\n",
    "    loss = criterion(outputs, data_out_data)\n",
    "    print(\"loss: \", loss)\n",
    "#    break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of COLAB_RNN-VAE-ODE_clean.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}